---
layout: post
title: Bayesian Network 요약
date: 2021-05-27
categories: [Bayesian]
tag: [causal-inference, bayesian, DAG, probability, gaussian-bayesian-network,data-analysis]
comments: true
---

만약에 the amount of data is not enough to induce a high scoring network.
해결책으로 Efron's Bootstrap a� a compu­ tationally efficient approach for answering these questions. 

네트워크를 더 높은 score로 얻기 위해서는 when learning structure, we can use prior knowledge on the structures we are searching to reduce the �ize of the search space, and thus improve both the speed of mducuon and more importantly, the quality of the learned network. Commonly used prior information include order­ ing constraints on the random variables, or the existence of certain arcs. 즉, constrain the search process을 주는 것이다.

특히, for small training sets we can find slightly better scoring networks using the constraints generated by the bootstrap.

과연 constraint-based와 score-based algorithms중에 어떤 방법이 더 accurate structural reconstruction할 것인가?

그리고 e hybrid algorithms more accurate than constraint-based 또는score-based algorithms

Constraint-based algorithms은 conditional independence constraints을 statistical tests로 찾는 방법이다.

Score-based algorithms은 여러 DAG을 생성한 다음 가장 큰 network score을 찾는 방법이다.

Hybrid algorithms은 constraint-based을 우선 사용해서 DAGs 후보들의 space를 줄이고 나서 score-based strategy로 가장 큰 score를 찾는 방법이다.   

데이타 타입

Discrete Bayesian Networks와 Gaussian Bayesian Networks로 나눌 수 있다. 

평가 
The metrics that measure structural discrepancies between graphs generally produce a score
that corresponds to some difference between the learnt graph and the ground truth graph.
One of the most commonly used metrics in this field of research is the Structural Hamming
Distance (SHD) proposed by Tsamardinos et al (2006). The SHD score represents the
minimum number of edge insertions, deletions, and arc reversals needed to convert the learnt
graph into the true graph. It turns out that
𝑆𝐻𝐷 = 𝐹𝑁 + 𝐹𝑃 (3)
where arc reversals fall either under 𝐹𝑁 or 𝐹𝑃.


우선 Discrete networks에서는score-based algorithms often have higher SHDs for small samples

constraint-based algorithms have better SHD than score-based algorithms for small sample sizes

Gaussian networks에서는 tabu search and simulated annealing have larger SHDs than
constraint-based or hybrid algorithms for most samples;
• hybrid and constraint-based algorithms have roughly the same SHD
for all sample sizes.

결론은 constraint-based algorithms are more accurate than score-based algorithms for small sample sizes
그리고 accuracy를 비교하자면 s hybrid algorithms이 딱히 뛰어나지 않았다. 


Bayesian network는 (G, $\theta$)로 되어 있다. G는 directed acyclic graph를 말하고  data set D with n observations으로 구성되어 있다. 


$\theta$는 network를 구성하는parameters set을 말한다.

You can use $$\LaTeX$$ to typeset formulas. A formula can be displayed inline, e.g. $$e=mc^2$$, or as a block:
$$\int_\Omega \nabla u \cdot \nabla v~dx = \int_\Omega fv~dx$$
Also check out this [LaTeX introduction](https://en.wikibooks.org/wiki/LaTeX/Mathematics).

출처: Data Analysis with Bayesian Networks: A Bootstrap Approach
Who Learns Better Bayesian Network Structures Constraint-Based, Score-based or Hybrid Algorithms?
