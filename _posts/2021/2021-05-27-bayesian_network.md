---
layout: post
title: Bayesian Network ìš”ì•½
date: 2021-05-27
#categories: [Bayesian]
#tag: [causal-inference, bayesian, DAG, probability, gaussian-bayesian-network,data-analysis]
comments: true
---

Bayesian network modellingì€ í¬ê²Œ 2ê°€ì§€ì˜ categoryë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. 

* **Discrete Bayesian networks**: globalê³¼ local distributionsì´ multinomiaìœ¼ë¡œ ê°€ì •ë˜ëŠ” ê²½ìš° ì‚¬ìš©ëœë‹¤. association measuresë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€  mutual information (log-likelihood ratio) and
Pearsonâ€™s X2ì´ë‹¤. 

* **Gaussian Bayesian networks**: global distribution is multivariate normalì´ê³   local distributions are univariate normals linked by linear dependence relationships. Association is measured by various estimators of Pearsonâ€™s correlation.



Gaussian Bayesian networks

* ìš°ì„  Gaussian Bayesian NetworkëŠ” ë…¸ë“œê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ì“¸ ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

* ê·¸ë¦¬ê³  parentsê°€ ì—†ëŠ” nodeì¦‰, root nodeëŠ” marginal distributionsë¡œ ì„¤ëª…ëœë‹¤. 

* ê° each node has a variance that is specific to that node and does not depend on the values of the parents

* ê° nodeì˜ local distributionì€ interceptì„ í¬í•¨í•˜ê³  interaction termì´ ì—†ëŠ” Gaussian linear modelë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. 


ë¨¼ì €, ì‰¬ìš´ **2. ëª¨ìˆ˜ ì¶”ì •** ë°©ë²•ì— ëŒ€í•´ ë¨¼ì € ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
GBNì€ $i$ë²ˆì§¸ ë…¸ë“œì¸ $X_i$ê°€ localí•˜ê²Œ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ ê°€ì •í•©ë‹ˆë‹¤.

   $$
   X_{i}=\mu_{X_{i}}+\Pi_{X_{i}} \boldsymbol{\beta}_{X_{i}}+\varepsilon_{X_{i}}, \quad \varepsilon_{X_{i}} \sim N\left(0, \sigma_{X_{i}}^{2}\right)
   $$


ë§Œì•½ì— the amount of data is not enough to induce a high scoring network.
í•´ê²°ì±…ìœ¼ë¡œ Efron's Bootstrap aï¿½ a compuÂ­ tationally efficient approach for answering these questions. 

ë„¤íŠ¸ì›Œí¬ë¥¼ ë” ë†’ì€ scoreë¡œ ì–»ê¸° ìœ„í•´ì„œëŠ” when learning structure, we can use prior knowledge on the structures we are searching to reduce the ï¿½ize of the search space, and thus improve both the speed of mducuon and more importantly, the quality of the learned network. Commonly used prior information include orderÂ­ ing constraints on the random variables, or the existence of certain arcs. ì¦‰, constrain the search processì„ ì£¼ëŠ” ê²ƒì´ë‹¤.

## ìƒë¬¼í•™ì  ë°ì´í„° ì ìš©ì˜ ë¬¸ì œì 

* ì¼ë°˜ì ìœ¼ë¡œ small sizes of available data sets (n  p) sample size increases, the information present in
the data dominates the information provided in the prior and determines the overall behaviour of the model. For small sample sizes:



* ê³¼ì—°  discrete and Gaussian assumptions really sensible for these kinds of data ì¦‰, Gene expression data are modelled as continuous random variables either assuming a Gaussian distribution or applying results from robust statistics.

* batch effects introduced by the instruments and the chemical reactions used in collecting the data.

---
## í•´ê²°ë²•

 Inference procedures are
usually unable to identify a single best BN, settling instead on a set
of equally well behaved models. For this reason, it is important to
incorporate prior biological knowledge into the network through the
use of informative priors

the prior distribution plays a much larger role because there is
not enough data available to disprove the assumptions the prior
encodes;


![](/images/feature_selected.png){: width = "400", height="300"}

ê·¸ë¦¬ê³  for sequence data, we aim to find the subset of genes

---
## bootstrapping

sample sizeê°€ì¶©ë¶„íˆ large í•˜ë©´ ë¬¸ì œê°€ ì—†ì§€ë§Œ
íŠ¹íˆ, for small training sets we can find slightly better scoring networks using the constraints generated by the bootstrap.
(non-parametric) bootstrap that doesn't make any distributional assumptions
í•˜ì§€ë§Œ If you are provided with small sample size (as a sidelight, what is "small" seems to depend on some underlying customary rule in each research field), no bootstrap will do the magic.
ì˜ˆë¥¼ë“¤ì–´ Assuming a database contains three observations for each of the two variables under investigation, no inference will make sense.

Bootstrap works well in small sample sizes by ensuring the correctness of tests (e.g. that the nominal 0.05 significance level is close to the actual size of the test), however the bootstrap does not magically grant you extra power. If you have a small sample, you have little power, end of story.

For prediction, bootstrapping will give you better (more honest) estimates of internal validity than split sample validation.


ê³¼ì—° constraint-basedì™€ score-based algorithmsì¤‘ì— ì–´ë–¤ ë°©ë²•ì´ ë” accurate structural reconstructioní•  ê²ƒì¸ê°€?

ê·¸ë¦¬ê³  e hybrid algorithms more accurate than constraint-based ë˜ëŠ”score-based algorithms

Constraint-based algorithmsì€ conditional independence constraintsì„ statistical testsë¡œ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.

Score-based algorithmsì€ ì—¬ëŸ¬ DAGì„ ìƒì„±í•œ ë‹¤ìŒ ê°€ì¥ í° network scoreì„ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.

Hybrid algorithmsì€ constraint-basedì„ ìš°ì„  ì‚¬ìš©í•´ì„œ DAGs í›„ë³´ë“¤ì˜ spaceë¥¼ ì¤„ì´ê³  ë‚˜ì„œ score-based strategyë¡œ ê°€ì¥ í° scoreë¥¼ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.   




ë°ì´íƒ€ íƒ€ì…

Discrete Bayesian Networksì™€ Gaussian Bayesian Networksë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. 

í‰ê°€ 
The metrics that measure structural discrepancies between graphs generally produce a score
that corresponds to some difference between the learnt graph and the ground truth graph.
One of the most commonly used metrics in this field of research is the Structural Hamming
Distance (SHD) proposed by Tsamardinos et al (2006). The SHD score represents the
minimum number of edge insertions, deletions, and arc reversals needed to convert the learnt
graph into the true graph. It turns out that
ğ‘†ğ»ğ· = ğ¹ğ‘ + ğ¹ğ‘ƒ (3)
where arc reversals fall either under ğ¹ğ‘ or ğ¹ğ‘ƒ.


ìš°ì„  Discrete networksì—ì„œëŠ”score-based algorithms often have higher SHDs for small samples

constraint-based algorithms have better SHD than score-based algorithms for small sample sizes

Gaussian networksì—ì„œëŠ” tabu search and simulated annealing have larger SHDs than
constraint-based or hybrid algorithms for most samples;
â€¢ hybrid and constraint-based algorithms have roughly the same SHD
for all sample sizes.

ê²°ë¡ ì€ constraint-based algorithms are more accurate than score-based algorithms for small sample sizes
ê·¸ë¦¬ê³  accuracyë¥¼ ë¹„êµí•˜ìë©´ s hybrid algorithmsì´ ë”±íˆ ë›°ì–´ë‚˜ì§€ ì•Šì•˜ë‹¤. 


Bayesian networkëŠ” (G, $\theta$)ë¡œ ë˜ì–´ ìˆë‹¤. GëŠ” directed acyclic graphë¥¼ ë§í•˜ê³   data set D with n observationsìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 


$\theta$ëŠ” networkë¥¼ êµ¬ì„±í•˜ëŠ”parameters setì„ ë§í•œë‹¤.

You can use $$\LaTeX$$ to typeset formulas. A formula can be displayed inline, e.g. $$e=mc^2$$, or as a block:
$$\int_\Omega \nabla u \cdot \nabla v~dx = \int_\Omega fv~dx$$
Also check out this [LaTeX introduction](https://en.wikibooks.org/wiki/LaTeX/Mathematics).

ì¶œì²˜: Data Analysis with Bayesian Networks: A Bootstrap Approach
Who Learns Better Bayesian Network Structures Constraint-Based, Score-based or Hybrid Algorithms?
https://stats.stackexchange.com/questions/112147/can-bootstrap-be-seen-as-a-cure-for-the-small-sample-size
https://www.bnlearn.com/examples/

