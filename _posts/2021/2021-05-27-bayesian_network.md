---
layout: post
title: Bayesian Network ìš”ì•½
date: 2021-05-27
#categories: [Bayesian]
#tag: [causal-inference, bayesian, DAG, probability, gaussian-bayesian-network,data-analysis]
comments: true
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    code_folding: hide
---


ì´ë²ˆ postëŠ” **Gene regulatory network (GRN)**ë¥¼ modelingì—ì„œ **Bayesian network (BN)**ì´ ì–´ë–»ê²Œ ì ìš©ë˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ ë³´ì•˜ë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ Bulk dataì˜ GRNì„ ìœ„í•´ì„œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ BN ì™¸ì—ë„ ì£¼ë¡œ correlation, regression, ordinary differential equations (ODEs), mutual information (MI), Gaussian graphical models ê·¸ë¦¬ê³  BNì´ë‹¤. ê°ê°ì˜ ë°©ë²•ë“¤ì€ ì•„ë˜ì™€ ê°™ì´ ì„œë¡œ ë‹¤ë¥¸ pros and consê°€ ìˆë‹¤.  

* correlation-based methodëŠ” assumptionsì— dependentí•˜ì§€ ì•Šê³  computationally efficientí•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. ê·¸ë¦¬ê³  ë¹„ìŠ·í•œ functions ë˜ëŠ” regulationë˜ëŠ” group genesì„ ì°¾ëŠ”ë° ìš©ì´í•˜ë‹¤. ë°˜ë©´ directionalityë¥¼ ì•Œ ìˆ˜ ì—†ê³  ë‹¤ë¥¸ informationì˜ integrationì„ í†µí•´ interpretabilityë¥¼ ë†’ì¼ ìˆ˜ê°€ ì—†ë‹¤. (WGCNA)

* Regression-based methodëŠ” linear cascadesì— ëŒ€í•´ì„œëŠ” ì˜ modelingì„ í•˜ì§€ë§Œ feed-forward loopsì— ëŒ€í•´ì„œëŠ” ê·¸ë ‡ì§€ ëª»í•˜ë‹¤. (GENIE3)

* MI ë°©ì‹ì€ genesì˜ pairsê°„ì˜ degree of dependenciesì— ì˜í•´ì„œ network structureê°€ ê²°ì •ëœë‹¤. directionalityì™€ potential causalityë¥¼ ì˜ ì¶”ë¡ í•  ìˆ˜ ìˆê³  Regression-based methodê³¼ ë°˜ëŒ€ë¡œ feed-forward loopsì— ëŒ€í•´ì„œ ë†’ì€ ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚´ê³  ë°˜ë©´ linear cascadesì— ëŒ€í•´ì„œëŠ” limited performanceë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 

* BN ë°©ì‹ì€ prior informationì„ integrationí•˜ê¸°ê°€ ìš©ì´í•˜ê³  causal/directional geneâ€“gene interactionsì„ ì¶”ë¡ í•  ìˆ˜ê°€ ìˆë‹¤. ë‹¨ì ìœ¼ë¡œëŠ” higher computation costì™€ optimal topologyë¥¼ ì°¾ëŠ” í™•ë¥ ì´ ì¡°ê¸ˆ ë–¨ì–´ì§„ë‹¤. ë‹¤ë¥¸ ë°©ì‹ì˜ í‘œí˜„ìœ¼ë¡œ large p & small n problemìœ¼ë¡œ structure learningì‹œì˜ genesì˜ ìˆ«ìê°€ ë„ˆë¬´ ë§ì€ NP-hardì˜ ë¬¸ì œì— ì§ë©´í•˜ê¸°ì— ë‹¤ë¥¸ ë°©ë²•ë“¤ê³¼ ë¹„êµí•´ì„œ ì ì€ ìˆ˜ì˜ small networksë¥¼ êµ¬ì„±í•  ìˆ˜ë°–ì— ì—†ë‹¤.     


---

## BN ì¹´í…Œê³ ë¦¬

Bayesian network modellingì€ í¬ê²Œ 2ê°€ì§€ì˜ categoryë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. 

* **Discrete Bayesian networks**: globalê³¼ local distributionsì´ multinomiaìœ¼ë¡œ ê°€ì •ë˜ëŠ” ê²½ìš° ì‚¬ìš©ëœë‹¤. association measuresë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€  mutual information (log-likelihood ratio) and
Pearsonâ€™s X2ì´ë‹¤. 

* **Gaussian Bayesian networks**: global distribution is multivariate normalì´ê³  local distributionsì´ univariate normalsì´ì—¬ì„œ globalê³¼ localì´ linear dependence relationships ì—°ê²°ë˜ì–´ ìˆëŠ” ê²½ìš° ì‚¬ìš©í•œë‹¤. Associationì€ Pearsonâ€™s correlationì´ ì‚¬ìš©ëœë‹¤.


Gaussian Bayesian networks

* ìš°ì„  Gaussian Bayesian NetworkëŠ” ë…¸ë“œê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ì“¸ ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

* ê·¸ë¦¬ê³  parentsê°€ ì—†ëŠ” nodeì¦‰, root nodeëŠ” marginal distributionsë¡œ ì„¤ëª…ëœë‹¤. 

* ê° each node has a variance that is specific to that node and does not depend on the values of the parents

* ê° nodeì˜ local distributionì€ interceptì„ í¬í•¨í•˜ê³  interaction termì´ ì—†ëŠ” Gaussian linear modelë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. 


## ìƒë¬¼í•™ì  ë°ì´í„° ì ìš©ì˜ ë¬¸ì œì 

sample sizeê°€ì¶©ë¶„íˆ large í•˜ë©´ ë¬¸ì œê°€ ì—†ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„°ì˜ ì–‘ì´ ë¶€ì¡±í•´ì„œ (small sizes of available data sets) high scoring network ì–»ê¸° í˜ë“¤ë‹¤. 

ê²°êµ­ì—ëŠ” ì œê³µëœ informationì— ì˜í•´ì„œ ì§€ë°°ë˜ê³  modelì„ ì „ë°˜ì ìœ¼ë¡œ ê²°ì •í•œë‹¤. 

* ê³¼ì—°  discrete and Gaussian assumptionsì´ ì´ëŸ¬í•œ dataì— ì˜ ì ìš©ì´ ë  ê²ƒì¸ê°€í•˜ëŠ” ì˜ë¬¸ì´ ë“ ë‹¤. 

* batch effectsì´ ìˆì„ ìˆ˜ ìˆë‹¤.

## í•´ê²°ë²•

í•´ê²°ì±…ìœ¼ë¡œ **Bootstrapping**ì´ computationally íš¨ê³¼ì ì¸ approachì´ë‹¤. 

ë‹¤ë¥¸ë°©ë²•ìœ¼ë¡œëŠ” prior biological knowledgeë¥¼ í†µí•´ì„œ search spaceì˜ sizeë¥¼ ì¤„ì´ê³  learning structureë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.

ë°©ë²•ìœ¼ë¡œ random variablesì— ëŒ€í•´ì„œ ordering constraintsì„ ì£¼ê±°ë‚˜ certain arcì— ëŒ€í•´ì„œ ë¯¸ë¦¬ ì •ë³´ë¥¼ ì£¼ëŠ” ê²ƒì´ë‹¤. ì¦‰,  search processì— constrainì„ ì£¼ëŠ” ê²ƒì´ë‹¤. 

* ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ genesì˜ subsetìœ¼ë¡œë§Œ ëª¨ë¸ë§ì„ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤.


![](/images/feature_selected.png){: width = "500", height="400"}


## bootstrapping

bootstrap (non-parametric)ì€ ì•„ë¬´ëŸ° distributional assumptionsì´ ì—†ë‹¤. 





sample sizeê°€ì¶©ë¶„íˆ large í•˜ë©´ ë¬¸ì œê°€ ì—†ì§€ë§Œ



small training setsì—ì„œëŠ” bootstrapì— ì˜í•œ constraints scoring networksê°€ ë” ìš°ìˆ˜í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. 


íŠ¹íˆ, for small training sets we can find slightly better scoring networks using the constraints generated by the bootstrap.
í•˜ì§€ë§Œ If you are provided with small sample size (as a sidelight, what is "small" seems to depend on some underlying customary rule in each research field), no bootstrap will do the magic.
ì˜ˆë¥¼ë“¤ì–´ Assuming a database contains three observations for each of the two variables under investigation, no inference will make sense.

Bootstrap works well in small sample sizes by ensuring the correctness of tests (e.g. that the nominal 0.05 significance level is close to the actual size of the test), however the bootstrap does not magically grant you extra power. If you have a small sample, you have little power, end of story.

For prediction, bootstrapping will give you better (more honest) estimates of internal validity than split sample validation.


ê³¼ì—° constraint-basedì™€ score-based algorithmsì¤‘ì— ì–´ë–¤ ë°©ë²•ì´ ë” accurate structural reconstructioní•  ê²ƒì¸ê°€?

ê·¸ë¦¬ê³  e hybrid algorithms more accurate than constraint-based ë˜ëŠ”score-based algorithms

Constraint-based algorithmsì€ conditional independence constraintsì„ statistical testsë¡œ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.

Score-based algorithmsì€ ì—¬ëŸ¬ DAGì„ ìƒì„±í•œ ë‹¤ìŒ ê°€ì¥ í° network scoreì„ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.

Hybrid algorithmsì€ constraint-basedì„ ìš°ì„  ì‚¬ìš©í•´ì„œ DAGs í›„ë³´ë“¤ì˜ spaceë¥¼ ì¤„ì´ê³  ë‚˜ì„œ score-based strategyë¡œ ê°€ì¥ í° scoreë¥¼ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.   

* ê³¼ì—°  discrete and Gaussian assumptions really sensible for these kinds of data ì¦‰, Gene expression data are modelled as continuous random variables either assuming a Gaussian distribution or applying results from robust statistics.


## The constraint-based approach
use statistical or information measures to test the conditional independence (CI) between variables. These methods rely heavily on the threshold selected for CI tests. High-order CI tests using large condition sets may be unreliable with the limitation of data size.

## score approach
traverse all possible structures using certain search algorithms to find the optimal one that maximizes the scoring function. In addition, prior knowledge can be easily incorporated into the model through the prior probability term in the scoring function. The search methods are heuristic and do not guarantee global optimal in most cases in consideration of large search space.


ë°ì´íƒ€ íƒ€ì…

Discrete Bayesian Networksì™€ Gaussian Bayesian Networksë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. 

í‰ê°€ 
The metrics that measure structural discrepancies between graphs generally produce a score
that corresponds to some difference between the learnt graph and the ground truth graph.
One of the most commonly used metrics in this field of research is the Structural Hamming
Distance (SHD) proposed by Tsamardinos et al (2006). The SHD score represents the
minimum number of edge insertions, deletions, and arc reversals needed to convert the learnt
graph into the true graph. It turns out that
ğ‘†ğ»ğ· = ğ¹ğ‘ + ğ¹ğ‘ƒ (3)
where arc reversals fall either under ğ¹ğ‘ or ğ¹ğ‘ƒ.


* ê³¼ì—°  discrete and Gaussian assumptions really sensible for these kinds of data ì¦‰, Gene expression data are modelled as continuous random variables either assuming a Gaussian distribution or applying results from robust statistics.

ìš°ì„  Discrete networksì—ì„œëŠ”score-based algorithms often have higher SHDs for small samples

constraint-based algorithms have better SHD than score-based algorithms for small sample sizes

Gaussian networksì—ì„œëŠ” tabu search and simulated annealing have larger SHDs than
constraint-based or hybrid algorithms for most samples;
â€¢ hybrid and constraint-based algorithms have roughly the same SHD
for all sample sizes.

ê²°ë¡ ì€ constraint-based algorithms are more accurate than score-based algorithms for small sample sizes
ê·¸ë¦¬ê³  accuracyë¥¼ ë¹„êµí•˜ìë©´ s hybrid algorithmsì´ ë”±íˆ ë›°ì–´ë‚˜ì§€ ì•Šì•˜ë‹¤. 


Bayesian networkëŠ” (G, $\theta$)ë¡œ ë˜ì–´ ìˆë‹¤. GëŠ” directed acyclic graphë¥¼ ë§í•˜ê³   data set D with n observationsìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 


$\theta$ëŠ” networkë¥¼ êµ¬ì„±í•˜ëŠ”parameters setì„ ë§í•œë‹¤.

You can use $$\LaTeX$$ to typeset formulas. A formula can be displayed inline, e.g. $$e=mc^2$$, or as a block:
$$\int_\Omega \nabla u \cdot \nabla v~dx = \int_\Omega fv~dx$$
Also check out this [LaTeX introduction](https://en.wikibooks.org/wiki/LaTeX/Mathematics).

ì¶œì²˜: Data Analysis with Bayesian Networks: A Bootstrap Approach
Who Learns Better Bayesian Network Structures Constraint-Based, Score-based or Hybrid Algorithms?
https://stats.stackexchange.com/questions/112147/can-bootstrap-be-seen-as-a-cure-for-the-small-sample-size
https://www.bnlearn.com/examples/

